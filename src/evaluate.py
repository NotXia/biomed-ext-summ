from models.loader import loadModel
from datasets import load_from_disk
import torch
import numpy as np
import argparse
import os
import random
from metrics.rouge import evalROUGE
from metrics.logger import MetricsLogger
import sys
from models.LongformerSummarizer import LongformerSummarizer


"""
    Model evaluation.

    Parameters
    ----------
        model : Model|str

        datasets : DatasetDict

        splits : str[]
            Splits of the dataset to use

        args : any
            Additional arguments.
"""
def evaluate(model, datasets, splits, args={}):
    metrics = MetricsLogger()

    for split in splits:
        for i, document in enumerate(datasets[split]):
            summary = ""

            if model == "oracle":
                selected_sents = [sent for i, sent in enumerate(document["sentences"]) if document["labels"][i]]
                summary = "\n".join(selected_sents)
            elif model == "lead":
                selected_sents = document["sentences"][:args["lead"]]
                summary = "\n".join(selected_sents)
            else:
                model.eval()
                with torch.no_grad():
                    selected_sents, _ = model.summarizeSentences(document["sentences"], args["strategy"], args["strategy_args"])
                    summary = "\n".join(selected_sents)

            metrics.add("rouge", evalROUGE( [document["ref_summary"]], [summary] ))
            sys.stdout.write(f"\r{i+1}/{len(datasets[split])} ({split}) --- {metrics.format(['rouge'])}\033[K")
            sys.stdout.flush()

    sys.stdout.write("\r\033[K")
    print(f"{metrics.format(['rouge'])}")



if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Model evaluation")
    model_args = parser.add_mutually_exclusive_group(required=True)
    model_args.add_argument("--checkpoint", type=str, help="Path to the checkpoint of the model")
    model_args.add_argument("--oracle", action="store_true", help="Evaluate using the oracle (greedy selection)")
    model_args.add_argument("--lead", type=int, help="Evaluate using LEAD-n (select first n sentences)")
    parser.add_argument("--dataset", type=str, required=True, help="Path to a preprocessed extractive dataset")
    parser.add_argument("--splits", type=str, default="test", required=True, help="Splits of the dataset to use for the evaluation (e.g. test,validation)")
    strategy_args = parser.add_mutually_exclusive_group(required="--checkpoint" in sys.argv)
    strategy_args.add_argument("--strategy-length", type=int, help="Summary generated with a length upper bound")
    strategy_args.add_argument("--strategy-count", type=int, help="Summary generated by selecting a given number of sentences")
    strategy_args.add_argument("--strategy-ratio", type=float, help="Summary proportional to the size of the document")
    strategy_args.add_argument("--strategy-threshold", type=float, help="Summary generated by selecting sentences with a score greater or equal to the given value [0, 1]")
    args = parser.parse_args()

    torch.manual_seed(42)
    random.seed(42)
    np.random.seed(42)

    datasets = load_from_disk(args.dataset)
    model = None
    eval_args = {}
    if args.checkpoint is not None:
        print("-- Loading model --")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        checkpoint = torch.load(args.checkpoint, map_location=device)
        model = loadModel(checkpoint["model_name"]).to(device)
        model.load_state_dict(checkpoint["model_state_dict"])

        if not isinstance(model, LongformerSummarizer):
            torch.use_deterministic_algorithms(mode=True)
            if torch.cuda.is_available(): os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"

        if args.strategy_length is not None:
            eval_args["strategy"] = "length"
            eval_args["strategy_args"] = args.strategy_length
        elif args.strategy_count is not None:
            eval_args["strategy"] = "count"
            eval_args["strategy_args"] = args.strategy_count
        elif args.strategy_ratio is not None:
            eval_args["strategy"] = "ratio"
            eval_args["strategy_args"] = args.strategy_ratio
        elif args.strategy_threshold is not None:
            eval_args["strategy"] = "threshold"
            eval_args["strategy_args"] = args.strategy_threshold
        else:
            raise NotImplementedError
    elif args.oracle:
        model = "oracle"
    elif args.lead != None:
        model = "lead"
        eval_args["lead"] = args.lead


    print("-- Starting evaluation --")
    evaluate(
        model = model,
        datasets = datasets,
        splits = args.splits.split(","),
        args = eval_args
    )
